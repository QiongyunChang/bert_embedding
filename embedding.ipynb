{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embedding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flair\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iaGWB2lqEcfr",
        "outputId": "cf21299d-3299-466a-e168-652d25ab56da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.11.1-py3-none-any.whl (401 kB)\n",
            "\u001b[K     |████████████████████████████████| 401 kB 13.7 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.63.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Collecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.10.0+cu111)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 55.7 MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from flair) (0.5.1)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 63.6 MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 48.2 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.18.0)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.21.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.14.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (2.6.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (1.3.0)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.11.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.0.49)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.3)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, sqlitedict, langdetect, pptree, wikipedia-api\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=5d03cf5af3d91fbad8d98616aa93c5e96b42b07b7c6f62755e27ce3bbfe08c78\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=96131d7e88a5d2b04f26484fdff1cd88c40c700e0d81387cbfd33445b7eb3fca\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=1241fcbe43e0b1769e518b3c457d83fb12ed9e17935c7bbf13dbe64e526c60b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=bbf34e8ba3d16655574b83fc7ad2aa1f98fe19f49f8bbc94d61c4e933d9e59e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=bd62618fa628236de19d7efd471a5eecb337463c59f365023a48eb997e825c1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=1da981cab9806b4a1ba9f3a88ab6023c9bc8b05fdddcc470b2f81687dd8bcec2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=3204b63c295632935d32fa5eb078070a0d7492698f470374b06d808a197f7bed\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built gdown mpld3 overrides sqlitedict langdetect pptree wikipedia-api\n",
            "Installing collected packages: requests, importlib-metadata, sentencepiece, py4j, overrides, wikipedia-api, sqlitedict, segtok, pptree, mpld3, more-itertools, langdetect, konoha, janome, hyperopt, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.11.1 ftfy-6.1.1 gdown-3.12.2 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.5 requests-2.27.1 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 wikipedia-api-0.5.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata",
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngdT1JzRHTbe",
        "outputId": "b82860ac-d9b0-4fbc-f122-fcb2bdb2425e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 8.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jAlEvxsyHIGB"
      },
      "outputs": [],
      "source": [
        "# import wandb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import re\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import BertEmbeddings\n",
        "\n",
        "\n",
        "PRETRAINED_MODEL_NAME = \"bert-base-multilingual-cased\" \n",
        "file_path = \"data.csv\"\n",
        "\n",
        "text = \"This article try to explain the result obtain in a teach innovation project which join education and cultural heritage. take advantage of the hold the exhibition ??The Art of Clara Peeters ??( autumn, 2016 ), the first exhibition devote to a woman painter in the Prado Museum, we developped a interdisciplinary projectbased learn with student from Master in Secondary Teacher train, specialize in Social science, of the be? laga University. They have to organize several activity for student from secondary education who have the opportunity of know the role of the woman in the history and the art, other story, other geography, still life painting o the daily life. In this Project, there be apply this methodology: servicelearning, problem base learn, collaborative learn, cooperative learn and flip classroom. This projet would achieve the follow goal: the elimination of prejudice concern the perception of the master as a necessary formality, the enlargement of the practical dimension, the implementation of innovative methodology, the interdisciplinary approach, the teamwork and the knowledge the existence of a ??hide curriculum ??? 2018, Universidad Complutense de Madrid. All right reserve.\"\n",
        "\n",
        "#read the data\n",
        "class load_data():\n",
        "    def __init__(self, path):\n",
        "        super(load_data, self).__init__()\n",
        "        self.path = path\n",
        "\n",
        "    def data_load(self):\n",
        "        file = pd.read_csv(self.path)\n",
        "        abstract = file.loc[:,'Abstract_lemma'].values\n",
        "        return abstract\n",
        "\n",
        "\n",
        "class bert_model():\n",
        "    def __init__(self, text):\n",
        "        super(bert_model, self).__init__()\n",
        "        self.text = text\n",
        "\n",
        "    def bert(self):\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "        model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "        encoded_inputs = tokenizer(self.text, padding=True, truncation=True, return_tensors=\"pt\")                            \n",
        "        ids = encoded_inputs['input_ids']\n",
        "        mask = encoded_inputs['attention_mask']\n",
        "        output = model(ids, mask)\n",
        "        final_layer = output.last_hidden_state\n",
        "        print(final_layer)\n",
        "        \n",
        "        \n",
        "    def word_embedding(self):\n",
        "        string = ''.join(self.text)\n",
        "        s = Sentence(self.text)\n",
        "        bert_embedding = BertEmbeddings()\n",
        "        bert_embedding.embed(s)\n",
        "        for token in s:\n",
        "            print(token.embedding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/dataset/data.csv\"\n",
        "data = load_data(path)\n",
        "text = data.data_load()\n",
        "model = bert_model(text[1])\n",
        "model.word_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unwwdBCtMTQe",
        "outputId": "a89267cf-231e-42ad-dccb-d7d8f30fb42c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1391,  0.2538, -0.5389,  ..., -0.4767,  1.3827, -0.9987])\n",
            "tensor([-1.0994,  0.1948, -0.2257,  ...,  0.3568,  0.7565, -0.1836])\n",
            "tensor([ 0.0521,  0.2693, -0.4582,  ..., -0.8610,  1.3562,  0.2120])\n",
            "tensor([-0.9308, -0.2944, -0.7477,  ..., -0.4552,  0.6817,  0.0857])\n",
            "tensor([-0.2278,  0.2282,  0.1193,  ..., -0.3592, -0.1034, -0.2523])\n",
            "tensor([-0.4827,  0.3503,  0.0153,  ..., -0.9832,  1.3845, -0.4342])\n",
            "tensor([-0.8440, -0.1176,  0.2153,  ..., -0.9877,  1.3416,  0.3147])\n",
            "tensor([-0.8693, -0.4426,  0.7813,  ..., -1.3042,  0.9296, -0.2100])\n",
            "tensor([-0.7882, -0.6301,  0.0555,  ..., -1.6778,  0.3201,  0.8916])\n",
            "tensor([-0.2539, -0.6022, -0.0806,  ..., -1.8320, -0.2663,  0.5676])\n",
            "tensor([ 0.2352,  0.3128,  0.4911,  ..., -0.7645,  0.8871,  0.3768])\n",
            "tensor([ 0.3067,  0.4728,  0.9921,  ..., -1.2887,  0.1811, -0.2376])\n",
            "tensor([ 0.0492,  0.2367,  0.8794,  ..., -1.1637,  0.5889,  0.1639])\n",
            "tensor([-0.3282,  0.5400,  1.1334,  ..., -0.7951, -0.1373,  0.8047])\n",
            "tensor([-0.2863, -0.3721,  0.2555,  ..., -0.4900,  0.6091,  0.6396])\n",
            "tensor([-0.2689,  0.6967,  0.3317,  ...,  0.0964,  1.3577,  0.6716])\n",
            "tensor([-0.5837, -0.2144,  0.5027,  ..., -0.2028,  0.7994, -0.1175])\n",
            "tensor([-0.2872,  0.2606,  0.0235,  ..., -0.9472,  0.9444, -0.3637])\n",
            "tensor([ 0.2151, -0.0368,  0.9498,  ..., -0.9493, -0.7104,  0.4094])\n",
            "tensor([-1.2386,  0.2417,  0.1577,  ..., -0.4818,  0.8910, -1.1298])\n",
            "tensor([-0.1298,  0.2011,  0.2328,  ..., -0.6128,  1.3692, -1.2821])\n",
            "tensor([ 0.3302, -0.2431,  0.4073,  ..., -1.6616,  0.3019, -0.2294])\n",
            "tensor([ 0.0183, -0.1520, -0.1547,  ..., -0.7434,  0.6236, -0.0588])\n",
            "tensor([-0.7786, -0.0656,  0.5689,  ..., -0.3508,  1.2335,  0.5218])\n",
            "tensor([-0.5054, -0.5142,  0.5024,  ..., -1.5875,  0.0287, -1.0469])\n",
            "tensor([-0.1301,  0.0682, -0.2216,  ..., -0.5997,  0.7247, -0.7751])\n",
            "tensor([-0.2774, -0.0078,  0.3146,  ..., -0.9029,  0.5356, -1.3480])\n",
            "tensor([-0.7038, -0.3939, -0.1658,  ..., -1.0519,  1.2773, -1.0354])\n",
            "tensor([-1.0644, -0.2348,  0.2414,  ...,  0.2280,  0.8454,  0.2939])\n",
            "tensor([-0.7785, -0.1134,  0.3273,  ..., -0.9902,  0.9890, -0.7783])\n",
            "tensor([-0.4636, -0.7405,  0.0847,  ..., -0.1345,  0.6320, -1.7444])\n",
            "tensor([ 0.0500, -0.6644, -0.2764,  ..., -1.1121,  1.2512, -0.9557])\n",
            "tensor([-0.7861, -0.3660,  0.2582,  ..., -0.6909,  0.5257,  1.0557])\n",
            "tensor([ 0.3950, -0.6908,  0.4146,  ..., -0.1155, -0.4211,  1.5867])\n",
            "tensor([-0.0371,  0.1890,  0.4868,  ...,  0.5117,  0.1738, -1.1494])\n",
            "tensor([ 0.1860,  0.1088,  0.5642,  ..., -0.5668,  0.2981, -0.6321])\n",
            "tensor([-0.7278, -1.0481,  0.9919,  ..., -0.0510,  1.2621, -1.0910])\n",
            "tensor([-0.5323, -0.3633,  0.5203,  ...,  0.1530,  0.7447,  0.7510])\n",
            "tensor([-0.4614,  0.1642,  0.4299,  ..., -1.2745, -0.5650, -0.8181])\n",
            "tensor([-1.0472,  0.1656,  0.5580,  ..., -1.6541,  0.4849,  0.1156])\n",
            "tensor([-0.8065,  0.1136,  0.1257,  ..., -1.6337,  0.1974,  0.4773])\n",
            "tensor([-0.1593,  0.7296,  0.2553,  ..., -0.5398,  0.5060, -0.3803])\n",
            "tensor([-0.4946,  0.2920,  0.2467,  ...,  0.1905,  0.2996,  0.1825])\n",
            "tensor([-0.6851,  0.2822,  0.2227,  ..., -0.4951,  0.5381,  0.7302])\n",
            "tensor([-0.1105,  0.4271,  0.2495,  ..., -1.6018,  0.5214,  0.2561])\n",
            "tensor([ 0.1997,  0.6685,  0.5375,  ..., -1.1159,  0.5497,  0.3972])\n",
            "tensor([-0.0775,  0.3636,  0.3948,  ..., -1.2053, -0.1815, -0.6940])\n",
            "tensor([ 0.0991,  0.4496,  0.5140,  ..., -0.3569,  0.1653, -0.6619])\n",
            "tensor([-0.0828,  0.1447,  0.2968,  ..., -0.0534,  0.3561, -0.3454])\n",
            "tensor([-0.3895,  0.1944, -0.1328,  ..., -0.1483,  0.8553,  0.6967])\n",
            "tensor([-0.3304,  0.4069,  0.1265,  ...,  0.9264,  0.7537,  0.3349])\n",
            "tensor([ 0.1346,  0.6445, -0.1123,  ..., -1.0245, -0.0188,  0.0510])\n",
            "tensor([ 0.1836,  0.1693,  0.0274,  ..., -0.2003,  0.2613, -0.5390])\n",
            "tensor([-0.4684, -0.1612,  0.1732,  ..., -0.8891,  0.8660, -0.6816])\n",
            "tensor([-0.7020, -0.3559,  0.3499,  ..., -0.7889, -0.1952,  0.0605])\n",
            "tensor([-0.3173, -0.0547,  0.4813,  ..., -0.6690,  0.0744, -1.0120])\n",
            "tensor([ 0.3986, -0.1098,  0.6645,  ..., -1.1295,  0.2010, -0.3065])\n",
            "tensor([-0.1759,  0.4518,  0.0510,  ..., -0.5217,  0.0798, -0.3632])\n",
            "tensor([-0.6499, -0.1175,  0.6460,  ..., -0.8271,  0.4052, -0.5126])\n",
            "tensor([-0.6608,  0.1005,  0.4174,  ..., -0.6830,  0.8196, -0.0812])\n",
            "tensor([-1.0613,  0.3979,  0.3950,  ..., -1.0795,  1.3837,  1.6554])\n",
            "tensor([-1.1374,  0.5287,  0.1518,  ..., -0.8098,  0.9096,  0.5019])\n",
            "tensor([-0.9936,  0.3361,  0.3785,  ..., -0.4617,  1.0283, -0.5791])\n",
            "tensor([-0.9322,  0.0262,  0.6668,  ...,  0.1244,  0.0799, -0.3020])\n",
            "tensor([-0.0508,  0.2355,  0.3463,  ..., -0.6493,  0.7809, -0.2415])\n",
            "tensor([-1.0802,  0.2445, -0.1682,  ..., -1.0476,  0.4432, -0.0047])\n",
            "tensor([-0.3977,  0.3404,  0.4002,  ..., -1.9324,  1.1550, -0.2267])\n",
            "tensor([-0.3571,  0.3112,  0.5154,  ..., -1.7773,  0.4087,  0.1393])\n",
            "tensor([-1.0941,  0.4391,  0.5264,  ..., -1.6391,  0.3791, -0.5417])\n",
            "tensor([-0.4696, -0.0370,  0.2182,  ..., -0.5758,  0.8656, -0.3200])\n",
            "tensor([-0.0866,  0.3106,  0.4106,  ..., -0.4596,  0.3241, -0.8834])\n",
            "tensor([ 0.9267,  0.2005,  0.2585,  ..., -0.4971,  0.4507,  0.6014])\n",
            "tensor([-0.1949,  0.2842,  0.5760,  ..., -0.5573, -0.2598, -0.3997])\n",
            "tensor([-0.3206, -0.4529,  0.5689,  ..., -0.2685,  0.7938, -0.5162])\n",
            "tensor([-0.5270,  0.4192,  0.4219,  ...,  0.0645,  1.5512,  0.0195])\n",
            "tensor([ 0.3129, -0.0819,  0.2682,  ..., -0.3869,  0.7605,  1.9263])\n",
            "tensor([-0.2849,  0.6668,  0.5612,  ..., -1.6383,  0.4958, -0.8109])\n",
            "tensor([-0.5478,  0.4414,  0.8546,  ..., -1.7018, -0.1212, -0.9375])\n",
            "tensor([-1.0583, -0.1107,  0.8182,  ..., -0.3482,  0.3108,  0.2699])\n",
            "tensor([-0.3023,  0.2661,  0.4850,  ..., -0.7864,  0.7909, -0.3354])\n",
            "tensor([-0.3260,  0.2973,  0.6908,  ..., -0.3408,  1.1879, -0.1969])\n",
            "tensor([ 0.1087,  0.1999,  1.1943,  ..., -0.9433,  0.9406, -0.2414])\n",
            "tensor([-0.6261,  0.0483,  0.4834,  ..., -1.6497,  0.5534, -0.0967])\n",
            "tensor([-0.4750,  0.0943,  0.4581,  ..., -0.9481,  1.3114, -0.2791])\n",
            "tensor([ 0.5769,  0.2734, -0.1668,  ..., -1.0898,  0.3026,  0.8054])\n",
            "tensor([-0.2953,  0.5293,  0.7173,  ..., -1.1548,  0.7887,  0.5421])\n",
            "tensor([-0.8115,  0.5252,  0.6703,  ..., -1.7128, -0.1110, -1.1452])\n",
            "tensor([-1.4588,  0.4077,  1.1586,  ..., -0.6776,  1.0237, -0.9313])\n",
            "tensor([-1.1904,  0.0498,  0.3160,  ..., -1.1759,  1.0925, -0.2851])\n",
            "tensor([-0.7120,  0.3608,  0.8415,  ..., -0.6746,  0.7863,  0.5240])\n",
            "tensor([-0.6598,  0.7734,  1.0379,  ..., -0.5250,  0.9768,  1.1798])\n",
            "tensor([-0.7208, -0.4154,  0.7048,  ..., -0.6630,  0.7703, -0.0466])\n",
            "tensor([-0.7895,  0.4021,  0.2353,  ..., -1.4852,  0.2953,  0.4872])\n",
            "tensor([-1.1751, -0.1395, -0.4535,  ..., -1.6431, -0.5600,  1.4020])\n",
            "tensor([-0.5143,  0.3098,  0.0449,  ..., -1.4113,  0.3668, -0.5657])\n",
            "tensor([-0.7870,  0.2481,  0.5010,  ..., -0.1238,  1.1958,  0.6236])\n",
            "tensor([-1.0733,  0.1711,  0.2455,  ..., -0.7879,  1.0032, -0.1363])\n",
            "tensor([-0.7707,  0.1120,  0.9806,  ..., -0.6486,  1.1297, -0.2559])\n",
            "tensor([-0.1730,  0.2742,  0.7191,  ..., -0.3188,  0.8187,  0.3459])\n",
            "tensor([-0.5259,  0.1836,  0.5537,  ..., -0.7550,  0.9076,  0.8610])\n",
            "tensor([-0.9133,  0.5136,  1.1780,  ...,  0.2923,  0.3524,  0.8049])\n",
            "tensor([-0.7423, -0.7106,  0.6782,  ..., -0.6200,  0.6669, -0.0394])\n",
            "tensor([-0.3372, -0.3609,  0.5107,  ..., -0.4158,  0.0653,  0.3247])\n",
            "tensor([-0.9752, -0.3560,  0.5357,  ..., -0.3223, -0.1305,  0.8820])\n",
            "tensor([-1.0107, -0.7064,  0.0955,  ...,  0.0593,  0.9236,  0.1558])\n",
            "tensor([-1.1623, -0.2440,  0.2503,  ..., -0.6892,  0.9048, -0.5878])\n",
            "tensor([-0.6361,  0.0544,  0.0144,  ..., -0.1856,  0.9308,  0.1952])\n",
            "tensor([-0.3969, -0.1258,  0.6501,  ..., -0.2825,  0.3144,  0.5755])\n",
            "tensor([-0.8091, -1.0831,  0.4412,  ..., -0.5025,  0.6891,  0.5487])\n",
            "tensor([ 0.2329, -0.6800,  0.9604,  ..., -0.6455, -0.3654, -0.1006])\n",
            "tensor([-1.3706, -0.0348,  0.2801,  ...,  0.3823, -0.3343, -0.0414])\n",
            "tensor([-0.3577,  0.6056, -0.7407,  ...,  0.3174,  0.8242,  0.4997])\n",
            "tensor([-1.0203,  0.1061,  0.0131,  ...,  0.9888,  0.0433,  1.9251])\n",
            "tensor([-0.6512, -0.2076, -0.2703,  ...,  0.5630,  0.3219,  1.0971])\n",
            "tensor([-1.0565, -0.9151, -0.0245,  ...,  0.3642,  0.0753,  0.6939])\n",
            "tensor([-1.3319, -0.9505,  0.3069,  ...,  0.3292,  0.4678,  0.7950])\n",
            "tensor([-0.4855, -0.5308,  0.1673,  ...,  0.0997,  0.4050,  0.2858])\n",
            "tensor([-0.2204, -0.6356, -0.4178,  ...,  0.5139, -0.0555,  0.5202])\n",
            "tensor([-0.8408, -1.0817, -0.3167,  ..., -0.0279, -0.2912,  0.2590])\n"
          ]
        }
      ]
    }
  ]
}